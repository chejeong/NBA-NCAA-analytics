---
title: "NCAA-Analysis"
output: html_document
date: "2022-10-28"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



We download and load necessary packages below
```{r}
#install.packages("lmerTest")
#install.packages("sjPlot")
#install.packages("estimatr")
#install.packages('car')
#install.packages("corrplot")
#install.packages("MASS")
#install.packages("margins")
#install.packages("caret")
#install.packages("InformationValue")
#install.packages("ISLR")
#install.packages("WeMix")
#install.packages("Kendall")
#install.packages("tsbox")


library("lmerTest")
library("tidyr")
library("psycho")
library("sjPlot")
library("estimatr")
library("car")
library("corrplot")
library("MASS")
library("dplyr")
library("ggplot2")
library("margins")
library("caret") #confusion matrix
library("InformationValue") #confusion matrix
library("ISLR") #confusion matrix
library("WeMix") #Wald Test
library("Kendall") #Season Mann-Kendall test
library("xts") #convert to time series object
library("tsbox") #convert xts to ts
library(pROC)

```


We import the filtered NCAA boxscore below
```{r}

box <- read.csv("/Users/chejeong/Desktop/ncaa_boxscore_filtered2.csv")

```

We change the dependent variable as a factor
```{r}

box$win <- as.factor(box$win)

```

We select relevant variables
```{r}

box <- box %>% dplyr::select(win,season,three_points_att,three_points_made,
                    two_points_att,two_points_made,free_throws_att,
                    free_throws_made,free_throws_att, offensive_rebounds, 
                    defensive_rebounds, assists, 
                    turnovers, steals, blocks, personal_fouls, 
                    fast_break_pts, second_chance_pts, points_off_turnovers)

```


We filter out null values
```{r}

print(dim(box))
print(colSums(is.na(box)))

box <- box %>% drop_na()

#export
#write.csv(box,"/Users/chejeong/Desktop/FA2022/DA401/NBA-NCAA-analytics/Data/NCAA/ncaa_boxscore_filtered3.csv", row.names = FALSE)

```


### Mixed Effects Logistic Regression Assumption Test

We test the mixed effects logistic regression assumptions below:

Linearity: relationship between the natural log of these probabilities (when expressed as odds) and your predictor variable is linear.

No outliers: independent variables should not contain outliers

Multicollinearity: independent variables should not be substantially correlated to each other.

*Linearity*

We test the lineartiy of the data.


*Outliers*

We explore if there are any outliers in the independent variables of interest below.
                  
"three_points_made"    "three_points_att"    
[17] "three_points_pct"     "two_points_made"      "two_points_att"       "two_points_pct"      
[21] "free_throws_made"     "free_throws_att"      "free_throws_pct"      "offensive_rebounds"  
[25] "defensive_rebounds"   "rebounds"             "assists"              "turnovers"           
[29] "steals"               "blocks"               "personal_fouls"       "foulouts"            
[33] "points"               "fast_break_pts"       "second_chance_pts"    "points_off_turnovers"

```{r}

boxplot(box$three_points_made,ylab = "3PM")
boxplot(box$three_points_att,ylab = "3PA")
boxplot(box$two_points_made,ylab = "2PM")
boxplot(box$two_points_att,ylab = "2PA")
boxplot(box$free_throws_made,ylab = "FTM")
boxplot(box$free_throws_att,ylab = "FTA")
boxplot(box$offensive_rebounds,ylab = "OFR")
boxplot(box$defensive_rebounds,ylab = "DFR")
boxplot(box$assists,ylab = "AST")
boxplot(box$turnovers,ylab = "TO")
boxplot(box$steals,ylab = "STL")
boxplot(box$blocks,ylab = "BLK")
boxplot(box$fast_break_pts,ylab = "Fastbreak PTS")
boxplot(box$second_chance_pts,ylab = "Second Chance PTS")
abline(h=150, lty=2)
boxplot(box$points_off_turnovers,ylab = "PTS off TO")
abline(h=60, lty=2)

```

We filter the outliers below.

```{r}

box <- box %>% dplyr::filter(points_off_turnovers <= 60 & second_chance_pts < 150)

```


*Boxplot of Mean 3PT stats through years*

```{r}

#boxplot of three point attempts throughout seasons
boxplot(box$three_points_att~box$season,main="Three Point Attempts 2013-2017",ylab = "Three Point Attempts", xlab="Season")

#generate three point percentage
box$three_points_percentage <- (box$three_points_made/box$three_points_att)

#boxplot of three point percentage throughout seasons
boxplot(box$three_points_percentage~box$season,main="Three Point Percentage 2013-2017",ylab = "Three Point Percentage", xlab="Season")

#boxplot of three point percentage throughout seasons
boxplot(box$three_points_made~box$season,main="Three Point Made Shots 2013-2017",ylab = "Three Point Made Shots", xlab="Season")

```



*Multicollinearity*

We test multicollinearity with the code below

```{r}

#fit linear model below
lm <- glm(win ~ three_points_att + three_points_made + two_points_att + two_points_made + free_throws_att + free_throws_made + offensive_rebounds + defensive_rebounds + assists + turnovers + steals + blocks + personal_fouls + fast_break_pts + second_chance_pts + points_off_turnovers,data=box, family = "binomial")

```

We test for multicollinearity using vif below.

```{r}

vif(lm)

```


```{r}

data_x <- box %>% dplyr::select(three_points_att, three_points_made, two_points_att,
                    two_points_made,free_throws_att,free_throws_made,
                    offensive_rebounds, defensive_rebounds, assists, 
                    turnovers, steals, blocks, personal_fouls, 
                    fast_break_pts, second_chance_pts, points_off_turnovers)

corrplot(cor(data_x), method = 'shade',is.corr=F, number.cex=0.4, addCoef.col="black")

```


```{r}

# Load the data
# Fit the logistic regression model
model <- glm(win ~ three_points_att + three_points_made + two_points_att + free_throws_made + offensive_rebounds + defensive_rebounds + assists + turnovers + steals + blocks + personal_fouls + fast_break_pts + second_chance_pts + points_off_turnovers,data=box, family = "binomial")
# Predict the probability (p)

probabilities <- predict(model, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")


# Select only numeric predictors
mydata2 <- box %>%
  dplyr::select_if(is.numeric) 
predictors <- colnames(mydata2)
# Bind the logit and tidying the data for plot
mydata2 <- mydata2 %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

ggplot(mydata2, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")

```


### Model Fitting

We fit the Mixed Effects Logistic Regression below

```{r}

m <- glmer(win ~ three_points_att + three_points_made + two_points_att + free_throws_made + offensive_rebounds + defensive_rebounds + assists + turnovers + steals + blocks + personal_fouls + fast_break_pts + second_chance_pts + points_off_turnovers + (1 | season),data=box, family = binomial,control = glmerControl(optimizer = "bobyqa"))

```

```{r}

#gives same result without optimizer
mtest <- glmer(win ~ three_points_att + three_points_made + two_points_att + free_throws_made + offensive_rebounds + defensive_rebounds + assists + turnovers + steals + blocks + personal_fouls + fast_break_pts + second_chance_pts + points_off_turnovers + (1 | season),data=box, family = binomial)

```

```{r}

summary(m)

```


We calculate the Average Marginal Effect (AME) below.
```{r}

modelMargins <- margins(m)

```

show the results of AME
```{r}
AMETable <- summary(modelMargins)
AMETable
```
We calculate the Marginal Effects at Means (MEM) below.

```{r}

estimates <- as.numeric(summary(m)$coefficients[,1])

```

```{r}

meanDf <- box %>% dplyr::select(three_points_att,three_points_made,two_points_att,
                         free_throws_made,offensive_rebounds,defensive_rebounds,
                         assists,turnovers,steals,blocks,personal_fouls,fast_break_pts,
                         second_chance_pts,points_off_turnovers) %>%  summarize_if(is.numeric, mean)
meanDf <- as.numeric(meanDf)
meanDf <- c(1,meanDf) #add 1 or intercept later for dot product
meanDf

```

We calculate the dot product to find the Y hat value below
```{r}

estimates %*% meanDf

```

We calculate the probability based off MEM calculation below
```{r}

MEMProb <- 1/(1+exp(-1*(estimates %*% meanDf)))
MEMProb

```
We calculate the MEM value below

```{r}

-0.356332*MEMProb*(1-MEMProb) #three point attempts
0.397776*MEMProb*(1-MEMProb) #three point made

```


### Model Evaluation

We perform the Wald Test to observe significance of the model.

```{r}

m2 <- update(mtest, ~.-three_points_att)
anova(m,m2)

m3 <- update(m2, ~.-three_points_made)
anova(m,m3)

```



```{r}

#split dataset into training and testing set
set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(box), replace=TRUE, prob=c(0.7,0.3))
train <- box[sample, ]
test <- box[!sample, ]

mt <- glmer(win ~ three_points_att + three_points_made + two_points_att + free_throws_made + offensive_rebounds + defensive_rebounds + assists + turnovers + steals + blocks + personal_fouls + fast_break_pts + second_chance_pts + points_off_turnovers + (1 | season),data=train, family = binomial,control = glmerControl(optimizer = "bobyqa"))

#use model to predict probability of default
predicted <- predict(mt, test, type="response")


test$win <- ifelse(test$win==1, 1, 0)

#find optimal cutoff probability to use to maximize accuracy
optimal <- optimalCutoff(test$win, predicted)[1]

#create confusion matrix
confusionMatrix(test$win, predicted)

#calculate sensitivity
cat("sensitivity: ",sensitivity(test$win, predicted), "\n")

#calculate specificity
cat("specificity: ",specificity(test$win, predicted), "\n")

#calculate total misclassification error rate
misClassError(test$win, predicted, threshold=optimal)

```


### Player Boxscore

```{r}

player <- read.csv("/Users/chejeong/Desktop/player_boxscore.csv")

```

We clean the data for the player boxscore data below.


Filtering criteria:

1. Player must play in the game

```{r}

player <- player %>% dplyr::select(season, player_id, height, played, starter, minutes_int64, position,
                         three_points_made, three_points_att, three_points_pct, two_points_made,
                         two_points_att, two_points_pct, free_throws_made, free_throws_att,
                         free_throws_pct, offensive_rebounds, defensive_rebounds, rebounds, assists,
                         turnovers, steals, blocks, personal_fouls, points, minutes_int64, weight) %>% filter(played == "true")

```


```{r}

print(dim(player))
print(colSums(is.na(player)))

player <- player %>% drop_na()

print(dim(player))

```

We generate dummy variables for the position below
```{r}

player$guard <- ifelse(grepl('G',temp$position,fixed=TRUE), 1, 0)
player$forward <- ifelse(grepl('F',temp$position,fixed=TRUE), 1, 0)

```

convert dummy variables as factor
```{r}
player$guard <- as.factor(player$guard)
player$forward <- as.factor(player$forward)

```


```{r}
player2 <- player

player2 <- player2 %>% dplyr::filter(starter == "true")

```


We construct the linear regression mixed effect model below.

```{r}

mp <- lmer(three_points_pct ~ two_points_att + two_points_made + free_throws_made + free_throws_att + offensive_rebounds + defensive_rebounds + assists + turnovers + steals + blocks + guard + forward + height + weight + minutes_int64 + (1 | season),data=player)

```

```{r}

summary(mp)

```

```{r}

mp2 <- lmer(three_points_att ~ two_points_att + two_points_made + free_throws_made + free_throws_att + offensive_rebounds + defensive_rebounds + assists + turnovers + steals + blocks + guard + forward + height + weight + minutes_int64 + (1 | season),data=player2)

```

```{r}

summary(mp2)

```


```{r}
tab_model(mp,mp2)
```


```{r}
colnames(player)
```


```{r}

distance <- read.csv("/Users/chejeong/Desktop/threepointshotdistance.csv")

```


```{r}

p <- ggplot(distance, aes(x=date, y=distanceToBasket, group=1)) + geom_line()
p + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + ylab("Distance To Basket (Inches)") + ggtitle("Average Three Point Shot Distance 2013 - 2018")

ggsave("/Users/chejeong/Desktop/distanceplot.png")

```


We conduct the Seasonal Mann-Kendall test below.

```{r}

distance$date <- paste(distance$date,"-01",sep="")
distance$date <- as.Date(distance$date)

distance2 <- xts(x=distance$distanceToBasket, order.by = distance$date)

distance2 <- ts_ts(distance)

SeasonalMannKendall(distance2)

```




```{r}

plot(fitted(m), residuals(m), xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, lty = 2)
lines(smooth.spline(fitted(m), residuals(m)))

```


```{r}

ggplot(data.frame(eta=predict(m,type="link"),pearson=residuals(m,type="pearson")),
      aes(x=eta,y=pearson)) +
    geom_point() +
    theme_bw()

```

```{r}

roc(test$win,predicted,pot=TRUE)
par(pty = "s")
plot(roc(test$win,predicted,pot=TRUE, print.auc = TRUE))

```



